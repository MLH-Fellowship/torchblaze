(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{81:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return r})),a.d(t,"metadata",(function(){return o})),a.d(t,"toc",(function(){return i})),a.d(t,"default",(function(){return c}));var n=a(3),s=a(7),l=(a(0),a(103)),r={id:"mltests",title:"MLTests"},o={unversionedId:"mltests",id:"mltests",isDocsHomePage:!1,title:"MLTests",description:"TorchBlaze comes with a PyTorch-based model testing suite, which is essentially a set of methods that can be used to perform a variety of integrity checks on your PyTorch models before and during the model training process in a bit to ensure that no unexpected results show up in your model outputs.",source:"@site/docs/mltests.md",slug:"/mltests",permalink:"/torchblaze/docs/mltests",version:"current",sidebar:"someSidebar",previous:{title:"Setup a TorchBlaze ML Project",permalink:"/torchblaze/docs/setup"},next:{title:"APITests",permalink:"/torchblaze/docs/apitest"}},i=[{value:"Getting Started with MLTests",id:"getting-started-with-mltests",children:[]},{value:"Model Tests",id:"model-tests",children:[{value:"# mls.get_params",id:"-mlsget_params",children:[]},{value:"# mls.check_cuda",id:"-mlscheck_cuda",children:[]},{value:"# mls.check_nan",id:"-mlscheck_nan",children:[]},{value:"# mls.check_infinite",id:"-mlscheck_infinite",children:[]},{value:"# mls.check_smaller",id:"-mlscheck_smaller",children:[]},{value:"# mls.check_greater",id:"-mlscheck_greater",children:[]},{value:"# mls.check_gradient_smaller",id:"-mlscheck_gradient_smaller",children:[]},{value:"# mls.check_params_changing",id:"-mlscheck_params_changing",children:[]}]},{value:"Automated Test",id:"automated-test",children:[{value:"# mls.model_test",id:"-mlsmodel_test",children:[]}]}],m={toc:i};function c(e){var t=e.components,a=Object(s.a)(e,["components"]);return Object(l.b)("wrapper",Object(n.a)({},m,a,{components:t,mdxType:"MDXLayout"}),Object(l.b)("p",null,"TorchBlaze comes with a PyTorch-based model testing suite, which is essentially a set of methods that can be used to perform a variety of integrity checks on your PyTorch models before and during the model training process in a bit to ensure that no unexpected results show up in your model outputs. "),Object(l.b)("h2",{id:"getting-started-with-mltests"},"Getting Started with MLTests"),Object(l.b)("hr",null),Object(l.b)("p",null,"For using the set of automated model-testing methods available in the ",Object(l.b)("strong",{parentName:"p"},"mltests")," module, the first step is to import it. This can be achieved with this simple command:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"import torchblaze.mltests as mls\n")),Object(l.b)("p",null,"To get a list of all the methods available in the module, you can use the following command:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"dir(mls)\n")),Object(l.b)("p",null,"Here's the list of methods, along with their usage, provided in the ",Object(l.b)("strong",{parentName:"p"},"mltests")," package."),Object(l.b)("h2",{id:"model-tests"},"Model Tests"),Object(l.b)("hr",null),Object(l.b)("h3",{id:"-mlsget_params"},"# mls.get_params"),Object(l.b)("p",null,"Retrieves the list of all the named parameters associated with a model."),Object(l.b)("p",null,"Arguments:\nmodel::torch.nn.Module- Ideally the deep learning model, but can be a singel model layer/set of layers too."),Object(l.b)("p",null,"Returns:\nparam_list::list- List of all the named parameters in the model."),Object(l.b)("h4",{id:"usage"},"Usage:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"import torchblaze.mltests as mls\n\n# assuming that your model class object is stored in variable 'model' \nparam_list = mls.get_params(model)\n\nprint(param_list)\n")),Object(l.b)("p",null,"Each element in the param_list will be a tuple, the first element of which is the parameter name, and the second element is the tensor of parameters associated with the named parameter. "),Object(l.b)("hr",null),Object(l.b)("h3",{id:"-mlscheck_cuda"},"# mls.check_cuda"),Object(l.b)("p",null,"Checks if the training device is of type CUDA or not."),Object(l.b)("p",null,"Arguments:\nparams::torch.Tensor- The parameters associated with a model layer."),Object(l.b)("p",null,"Returns:\nNone: Throws an exception if the training device is not CUDA-enabled. "),Object(l.b)("h4",{id:"usage-1"},"Usage:"),Object(l.b)("p",null,"Here is a an example of how you can implement this test. In order to use it as a standalone test, you need to provide the method, as an argument, a tensor of named parameters associated with the model. It can be any named parameter associated with the model."),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"import torchblaze.mltests as mls\n\n# getting the list of named parameters of the model \nparam_list = mls.get_params(model)\n\n# getting a single named parameter tensor from the list\nparam = param_list[0][1]\n\n# performing the test\nmls.check_cude(param)\n\n")),Object(l.b)("p",null,"In case the model is not training on a CUDA-enabled device, you will get a ",Object(l.b)("inlineCode",{parentName:"p"},"DeviceNotCudaException")," exception. "),Object(l.b)("hr",null),Object(l.b)("h3",{id:"-mlscheck_nan"},"# mls.check_nan"),Object(l.b)("p",null,"Tests for the presence of NaN values ",'[torch.tensor(float("nan"))]'," in the given tensor of model parameter."),Object(l.b)("p",null,"Arguments:\nname::str- Name of the parameter\nparams::torch.Tensor- Trainable named parameters associated with a layer"),Object(l.b)("p",null,"Returns:\nNone- Throws an exception in case any parameter is a NaN value. "),Object(l.b)("h4",{id:"usage-2"},"Usage:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"import torchblaze.mltests as mls\n\n# getting the list of named parameters of the model \nparam_list = mls.get_params(model)\n\n# performing nan-value check on every named parameter \nfor name, param in param_list:\n    check_nan(name, param)\n\n")),Object(l.b)("p",null,"In case any model parameter is a NaN value you will get a ",Object(l.b)("inlineCode",{parentName:"p"},"NaNParamsException")," exception. "),Object(l.b)("hr",null),Object(l.b)("h3",{id:"-mlscheck_infinite"},"# mls.check_infinite"),Object(l.b)("p",null,"Tests for the presence of infinite values ",'[torch.tensor(float("Inf"))]'," in the given tensor of model parameter."),Object(l.b)("p",null,"Arguments:\nname::str- Name of the parameter\nparams::torch.Tensor- Trainable named parameters associated with a layer"),Object(l.b)("p",null,"Returns:\nNone- Throws an exception in case any parameter is a Inf value. "),Object(l.b)("h4",{id:"usage-3"},"Usage:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"import torchblaze.mltests as mls\n\n# getting the list of named parameters of the model \nparam_list = mls.get_params(model)\n\n# performing infinite-value check on every named parameter \nfor name, param in param_list:\n    check_infinite(name, param)\n\n")),Object(l.b)("p",null,"In case any model parameter is a infinite value you will get a ",Object(l.b)("inlineCode",{parentName:"p"},"InfParamsException")," exception. "),Object(l.b)("hr",null),Object(l.b)("h3",{id:"-mlscheck_smaller"},"# mls.check_smaller"),Object(l.b)("p",null,"Tests if the absolute value of any parameter exceeds a certain threshold."),Object(l.b)("p",null,"Arguments:\nname::str- Name of the parameter\nparams::torch.Tensor- Trainable named parameters associated with a layer\nupper_limit::float- The threshold value every parameter should be smaller than in terms of its absolute value."),Object(l.b)("p",null,"Returns:\nNone- Throws an exception in case any parameter is a exceeds threshold value. "),Object(l.b)("h4",{id:"usage-4"},"Usage:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"import torchblaze.mltests as mls\n\n# getting the list of named parameters of the model \nparam_list = mls.get_params(model)\n\n# performing the check on every named parameter \nfor name, param in param_list:\n    check_smaller(name, param, upper_limit=1e2)\n\n")),Object(l.b)("p",null,"In case any parameter exceeds a upper_limit threshold value you will get a ",Object(l.b)("inlineCode",{parentName:"p"},"ParamsTooLargeException")," exception. "),Object(l.b)("hr",null),Object(l.b)("h3",{id:"-mlscheck_greater"},"# mls.check_greater"),Object(l.b)("p",null,"Tests if the absolute value of any parameter falls below a certain threshold."),Object(l.b)("p",null,"Arguments:\nname::str- Name of the parameter\nparams::torch.Tensor- Trainable named parameters associated with a layer\nlower_limit::float- The threshold value every parameter should be greater than in terms of its absolute value."),Object(l.b)("p",null,"Returns:\nNone- Throws an exception in case any parameter is a NaN value. "),Object(l.b)("h4",{id:"usage-5"},"Usage:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"import torchblaze.mltests as mls\n\n# getting the list of named parameters of the model \nparam_list = mls.get_params(model)\n\n# performing the check on every named parameter \nfor name, param in param_list:\n    check_greater(name, param, lower_limit=1e-2)\n\n")),Object(l.b)("p",null,"In case any parameter falls below the lower_limit threshold value you will get a ",Object(l.b)("inlineCode",{parentName:"p"},"ParamsTooSmallException")," exception. "),Object(l.b)("hr",null),Object(l.b)("h3",{id:"-mlscheck_gradient_smaller"},"# mls.check_gradient_smaller"),Object(l.b)("p",null,"Tests if the absolute gradient value for any parameter exceed a certain threshold. Can be used to check for gradient explosion."),Object(l.b)("p",null,"Arguments:\nname::str- Name of the parameter\nparams::torch.Tensor- Trainable named parameters associated with a layer\ngrad_limit::float- A threshold value, such that, |params.grad| < grad_limit"),Object(l.b)("p",null,"Returns:\nNone- Throws an exception in case the gradient for any parameter exceeds the threshold value (grad_limit)\nOR\nNone- Throws an exception in case the method is used without running the loss.backward() method for backprop. "),Object(l.b)("h4",{id:"usage-6"},"Usage:"),Object(l.b)("div",{className:"admonition admonition-caution alert alert--warning"},Object(l.b)("div",{parentName:"div",className:"admonition-heading"},Object(l.b)("h5",{parentName:"div"},Object(l.b)("span",{parentName:"h5",className:"admonition-icon"},Object(l.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"},Object(l.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"}))),"caution")),Object(l.b)("div",{parentName:"div",className:"admonition-content"},Object(l.b)("p",{parentName:"div"},"Running this method for a model whose gradients have not been initialized (i.e., a model that has not gone training) will result in GradientsUninitializedException."))),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"import torchblaze.mltests as mls\n\n# getting the list of named parameters of the model \nparam_list = mls.get_params(model)\n\n# performing gradient check on every named parameter (assuming model has undergone at least one step of training) \nfor name, param in param_list:\n    check_gradient_smaller(name, params, grad_limit=1e4)\n\n")),Object(l.b)("p",null,"In case any parameter's gradient value exceeds the grad_limit threshold value, you will get a ",Object(l.b)("inlineCode",{parentName:"p"},"GradientAboveThresholdException")," exception. "),Object(l.b)("hr",null),Object(l.b)("h3",{id:"-mlscheck_params_changing"},"# mls.check_params_changing"),Object(l.b)("p",null,"Tests if the parameters in the model/certain layer are changing after a training cycle."),Object(l.b)("p",null,"Arguments:\nname::str- Name of the parameter\nparams_list_old::list- List of trainable model parameters BEFORE training cycle.\nparams_list_new::list- List of trainable model parameters AFTER training cycle."),Object(l.b)("p",null,"Returns:\nNone- Throws an exception in case the parameters are not changing "),Object(l.b)("h4",{id:"usage-7"},"Usage:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"import torchblaze.mltests as mls\n\n# getting the list of named parameters of the model BEFORE training\nparam_list_old = mls.get_params(model)\n\n...\n\n# getting the list of named parameters of the model AFTER training\nparam_list_new = mls.get_params(model)\n\n# performing the parameter-change test \ncheck_params_changing(params_list_old, params_list_new)\n\n")),Object(l.b)("p",null,"In case none of the model parameters change after training the model, you get a ",Object(l.b)("inlineCode",{parentName:"p"},"ParamsNotChangingException")," exception. "),Object(l.b)("h2",{id:"automated-test"},"Automated Test"),Object(l.b)("hr",null),Object(l.b)("p",null,"While the methods given above can be used to define your own model tests, the mltests module comes with an automated test too."),Object(l.b)("h3",{id:"-mlsmodel_test"},"# mls.model_test"),Object(l.b)("p",null,"Executes a suite of automated tests on the ML model.\nSet <test_name> = False if you want to exclude a certain test from the test suite."),Object(l.b)("p",null,"Arguments:\nmodel::nn.Module- The model that you want to test."),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre"},"batch_x::torch.Tensor- A single batch of data features to perform model checks\n\nbatch_y::torch.Tensor- A single batch of data labels to perform model checks\n\noptim_fn::torch.optim- default=torch.optim.Adam, Optimizer algorithm to be used during model training \n\nloss_fn- default=torch.nn.CrossEntropyLoss, Loss function to be used for model evaluation during training\n\ntest_gradient_smaller::bool- default=True, Asserts if gradients exceed a certain threshold  \n\ntest_greater::bool- default=True, Asserts if all parameters > threshold limit\n\ntest_smaller::bool- default=True, Asserts if all parameters < threshold limit\n\ntest_infinite::bool- default=True, Asserts that no parameters == infinite\n\ntest_nan::bool- default=True, Asserts that no parameters == NaN\n\ntest_params_changing::bool- Default=False, Asserts that all the parameters change/update after the training is complete\n\ntest_cuda::bool- Default=False, Asserts that the model is training on a cuda-enabled GPU\n\nupper_limit::float- default=1e2, Absolute value of all parameters should be smaller than this threshold value\n\nlower_limit::float- default=1e-2, Absolute value of all parameters should be greater than this threshold value\n\ngrad_limit::float- default=1e4, Absolute value of all gradients should be smaller than this threshold value \n")),Object(l.b)("h4",{id:"usage-8"},"Usage:"),Object(l.b)("pre",null,Object(l.b)("code",{parentName:"pre",className:"language-py"},"import torchblaze.mltests as mls\n\n# loss function\ncriterion = torch.nn.CrossEntropyLoss()\n\n# optimizer function\noptimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n\n# running automated tests\nmodel_test(model=net, batch_x=x, batch_y=y, optim_fn=optimizer, loss_fn=criterion)\n\n")),Object(l.b)("p",null,"In the example given above, we assume that the model class object is stored under that variable name ",Object(l.b)("inlineCode",{parentName:"p"},"net"),".\nx: A batch of training features\ny: A batch of training lables, corresponding to the feature batch x. "))}c.isMDXComponent=!0}}]);